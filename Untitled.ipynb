{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b07875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9818a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1 = [\"\"\"The all new Electric cars are so effiecient and not harmful for the environment , there are many leading companies inversting in recharagable cars Including , Tesla,TaTa and other indian companies as well.\"\"\" ]\n",
    "Doc2 = [\"\"\"Machine learning is now days a very popular field of study, a continuous reasearch is going on this , Machine learning is all about maths. Analysing the patters solve the task.\"\"\"]\n",
    "Doc3 = [\"\"\"public transport is good for city like mumbai and delhi, it helps to reduce the pollution and traffic. A wise person will always prefer public transport. taking a public transport also helps your pocket\"\"\"]\n",
    "Doc4 = [\"\"\"The man behind the wicket is MS DHONI , his fast hand behind wicket are a big advantage for india, but pant has now carrying the legacy of DHONI but he is not that fast\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b09db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The all new Electric cars are so effiecient and not harmful for the environment , there are many leading companies inversting in recharagable cars Including , Tesla,TaTa and other indian companies as well.',\n",
       " 'Machine learning is now days a very popular field of study, a continuous reasearch is going on this , Machine learning is all about maths. Analysing the patters solve the task.',\n",
       " 'public transport is good for city like mumbai and delhi, it helps to reduce the pollution and traffic. A wise person will always prefer public transport. taking a public transport also helps your pocket',\n",
       " 'The man behind the wicket is MS DHONI , his fast hand behind wicket are a big advantage for india, but pant has now carrying the legacy of DHONI but he is not that fast']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = Doc1+Doc2+Doc3+Doc4\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f40830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,\" \",''.join(text))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [tok.strip() for tok in tokens]\n",
    "    if is_lower_case:\n",
    "        cleaned_token = [tok for tok in tokens if tok not in stopword_list]\n",
    "    else:\n",
    "        cleaned_tokens = [tok for tok in tokens if tok.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(cleaned_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939a99f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new Electric cars effiecient harmful environment many leading companies inversting recharagable cars Including Tesla TaTa indian companies well'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(fin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a7593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = {}\n",
    "file = open('data1.txt', encoding = 'utf-8')\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vectors = np.asarray(values[1:])\n",
    "    glove_vectors[word] = vectors\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37c103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-0.29353', '0.33247', '-0.047372', '-0.12247', '0.071956',\n",
       "       '-0.23408', '-0.06238', '-0.0037192', '-0.39462', '-0.69411',\n",
       "       '0.36731', '-0.12141', '-0.044485', '-0.15268', '0.34864',\n",
       "       '0.22926', '0.54361', '0.25215', '0.097972', '-0.087305',\n",
       "       '0.87058', '-0.12211', '-0.079825', '0.28712', '-0.68563',\n",
       "       '-0.27265', '0.22056', '-0.75752', '0.56293', '0.091377',\n",
       "       '-0.71004', '-0.3142', '-0.56826', '-0.26684', '-0.60102',\n",
       "       '0.26959', '-0.17992', '0.10701', '-0.57858', '0.38161',\n",
       "       '-0.67127', '0.10927', '0.079426', '0.022372', '-0.081147',\n",
       "       '0.011182', '0.67089', '-0.19094', '-0.33676', '-0.48471',\n",
       "       '-0.35406', '-0.15209', '0.44503', '0.46385', '0.38409',\n",
       "       '0.045081', '-0.59079', '0.21763', '0.38576', '-0.44567',\n",
       "       '0.009332', '0.442', '0.097062', '0.38005', '-0.11881', '-0.42718',\n",
       "       '-0.31005', '-0.025058', '0.12689', '-0.13468', '0.11976',\n",
       "       '0.76253', '0.2524', '-0.26934', '0.068629', '-0.10071',\n",
       "       '0.011066', '-0.18532', '0.44983', '-0.57507', '0.12278',\n",
       "       '-0.064878', '0.044456', '-0.020999', '-0.069838', '-0.47329',\n",
       "       '-0.43074', '0.39158', '-0.047815', '-0.93659', '-0.55128',\n",
       "       '-0.1422', '-0.15829', '0.15623', '0.070461', '0.19892', '0.18942',\n",
       "       '-0.19339', '-0.46594', '-0.028825', '0.0056752', '-0.0054038',\n",
       "       '0.43144', '0.12257', '-0.2611', '0.04847', '0.32244', '-0.31064',\n",
       "       '-0.10559', '0.97954', '0.069626', '-0.023187', '-0.86293',\n",
       "       '0.48273', '0.23649', '-0.0034704', '-0.18932', '0.18588',\n",
       "       '0.023211', '-0.30643', '-0.35717', '0.19605', '-0.1584',\n",
       "       '-0.0058626', '0.35248', '0.036053', '-0.53933', '0.49435',\n",
       "       '0.45332', '-0.18477', '0.040648', '-0.094517', '-0.07116',\n",
       "       '0.74005', '-0.11465', '-0.26916', '0.089765', '-0.25205',\n",
       "       '-0.21469', '-0.38847', '0.32509', '0.25773', '-0.51764',\n",
       "       '-0.38457', '0.028254', '-0.21232', '-0.27311', '0.69178',\n",
       "       '-0.37681', '0.14241', '-0.24926', '0.40314', '-0.052916',\n",
       "       '0.07684', '0.2135', '0.10921', '0.049658', '0.02093', '0.11953',\n",
       "       '0.28648', '0.87791', '0.085838', '0.31983', '0.51856', '-0.22628',\n",
       "       '0.12402', '0.48805', '0.22111', '-0.52021', '0.0025106',\n",
       "       '-0.13305', '-0.052565', '0.32744', '0.64985', '0.072426',\n",
       "       '-0.52743', '-0.20913', '-0.27897', '-0.10834', '-0.10103',\n",
       "       '0.15299', '-0.36681', '0.082445', '0.1739', '-0.28099',\n",
       "       '-0.069136', '0.7895', '0.060571', '0.38693', '-0.16495',\n",
       "       '-0.21801', '0.33288', '-0.44568', '-0.49892', '-0.34438',\n",
       "       '-0.035606', '-0.24239', '-0.4747', '-0.17254', '0.071349',\n",
       "       '1.4091', '0.46166', '0.46546', '-0.30979', '0.37203', '0.47897',\n",
       "       '-0.28872', '-0.65515', '-0.13629', '-0.14287', '-0.04843',\n",
       "       '-0.12786', '0.18941', '-0.037051', '0.59471', '-0.0051618',\n",
       "       '-0.0086009', '-0.33313', '0.288', '-0.058965', '-0.67275',\n",
       "       '0.15544', '0.074187', '-0.36441', '-0.021285', '-0.065337',\n",
       "       '0.13827', '0.008395', '-0.041113', '0.29401', '-0.10344',\n",
       "       '-0.052371', '-0.63084', '0.16311', '0.052826', '-0.021797',\n",
       "       '-0.28115', '-0.078361', '-0.38124', '0.078089', '0.38411',\n",
       "       '-0.34629', '-0.4322', '0.091731', '-0.67867', '-0.041138',\n",
       "       '-0.53981', '0.10678', '0.03343', '0.81396', '-0.19448',\n",
       "       '0.026248', '-0.14215', '0.2954', '0.62738', '0.26499', '0.6191',\n",
       "       '-0.04113', '0.12301', '0.3158', '0.10698', '0.023654', '-0.41355',\n",
       "       '0.034852', '0.21361', '0.045834', '0.053415', '-0.36421',\n",
       "       '0.19707', '0.50916', '-0.1949', '-0.18788', '-0.24449',\n",
       "       '-0.63397', '-0.23125', '-0.18823', '-1.0601', '0.47794',\n",
       "       '-1.0102', '0.24604', '-0.4876', '0.79146', '-0.11047', '-0.21762',\n",
       "       '-0.6178', '0.27815', '-0.098169', '-0.063205', '0.066069',\n",
       "       '-0.69305', '-0.25928', '0.44591', '-0.64198', '-0.33084',\n",
       "       '-0.30154', '-0.56359', '0.60501', '-0.09673', '0.44444',\n",
       "       '0.22007'], dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bb2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dimension = 300\n",
    "def get_embedding(x):\n",
    "    arr  = np.zeros(vec_dimension)\n",
    "    text = str(x).split()\n",
    "    for t in text:\n",
    "        try:\n",
    "            vec = glove_vectors.get(t).astype(float)\n",
    "            arr = arr + vec\n",
    "        except:\n",
    "            pass\n",
    "    arr = arr.reshape(1,-1)[0]\n",
    "    return(arr/len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e9ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting average vector for each document\n",
    "out_dict = {}\n",
    "for sen in fin:\n",
    "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
    "    dict = { sen : (average_vector) }\n",
    "    out_dict.update(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7fa8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The all new Electric cars are so effiecient and not harmful for the environment , there are many leading companies inversting in recharagable cars Including , Tesla,TaTa and other indian companies as well.': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'Machine learning is now days a very popular field of study, a continuous reasearch is going on this , Machine learning is all about maths. Analysing the patters solve the task.': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'public transport is good for city like mumbai and delhi, it helps to reduce the pollution and traffic. A wise person will always prefer public transport. taking a public transport also helps your pocket': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'The man behind the wicket is MS DHONI , his fast hand behind wicket are a big advantage for india, but pant has now carrying the legacy of DHONI but he is not that fast': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397c037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(query_embedding, average_vector_doc):\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, \n",
    "    average_vector_doc))]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a4f8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ranked_documents(query):\n",
    "    query_words = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
    "    rank = []\n",
    "    for k,v in out_dict.items():\n",
    "        rank.append((k, get_sim(query_words, v)))\n",
    "        rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
    "    print('Ranked Documents :')\n",
    "    return rank[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8b87326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The all new Electric cars are so effiecient and not harmful for the environment , there are many leading companies inversting in recharagable cars Including , Tesla,TaTa and other indian companies as well.',\n",
       " [-0.2191694925464962])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ranked_documents(\"DF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbca60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3eb7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Machine learning is now days a very popular field of study, a continuous reasearch is going on this , Machine learning is all about maths. Analysing the patters solve the task.',\n",
       " [0.6735373306361353])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(\"machine learning \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869f245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_function = pickle.dumps(Ranked_documents)\n",
    "with open('Model.pkl', 'wb') as file:\n",
    "    pickle.dump(pickled_function, file)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
